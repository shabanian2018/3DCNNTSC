{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPIE 2022_Medical Imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc568314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade pip\n",
    "# !pip install --quiet unet==0.7.7\n",
    "# !pip install --quiet torchio==0.18.33\n",
    "# !apt -qq install tree\n",
    "#!pip install unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import scipy\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import enum\n",
    "import h5py\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "import matplotlib.image as img\n",
    "import nibabel as nb\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from matplotlib import style\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, basename, isdir\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "from matplotlib import style\n",
    "from nibabel.testing import data_path\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchio as tio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchio.transforms import HistogramStandardization\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# import monai\n",
    "# from monai.data import ImageDataset\n",
    "# from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, EnsureType\n",
    "# from torch.optim import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "print('Using device ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TorchIO version:', tio.__version__)\n",
    "print('Torch version:', tio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8836962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "DATA_DIR = \"D:/TSC/train_mix\"\n",
    "\n",
    "TRAIN_DIR_NAME = 'D:/TSC/train_mix/Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047eab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(TRAIN_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f482c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not entirely robust! The paths below DATA_DIR need to be like mine above.\n",
    "source_files = []\n",
    "#print(\"PATH:\", DATA_DIR)\n",
    "for current_file in Path(DATA_DIR).rglob('*.gz'):\n",
    "    #print(current_file)\n",
    "    source_files.append(str(current_file).split('\\\\')[-4:])\n",
    "    #print(source_files)\n",
    "source_files_df = pd.DataFrame(source_files)\n",
    "#source_files_df.columns = ['a', 'b', 'c', 'd', 'set', 'class', 'filename']\n",
    "source_files_df.columns = ['set','class', 'MRID', 'Sequences']\n",
    "\n",
    "print(source_files_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b499d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not entirely robust! The paths below DATA_DIR need to be like mine above.\n",
    "train_source_files = []\n",
    "#print(\"PATH:\", DATA_DIR)\n",
    "for current_file in Path(TRAIN_DIR_NAME).rglob('*.gz'):\n",
    "    #print(current_file)\n",
    "    train_source_files.append(str(current_file).split('\\\\')[-4:])\n",
    "    #print(source_files)\n",
    "train_source_files_df = pd.DataFrame(train_source_files)\n",
    "#source_files_df.columns = ['a', 'b', 'c', 'd', 'set', 'class', 'filename']\n",
    "train_source_files_df.columns = ['set','class', 'MRID', 'Sequences']\n",
    "\n",
    "print(train_source_files_df.head())\n",
    "\n",
    "# #*********************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3f4d5",
   "metadata": {},
   "source": [
    "# Counting MRI sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the number of MRI sequesnces/ what kind of MRI sequesces we want!\n",
    "\n",
    "#effective_sequences = source_files_df.loc[(source_files_df['Sequences'] == 'FSPGR BRAVO.nii.gz')]\n",
    "#effective_sequences = source_files_df.loc[(source_files_df['Sequences'] == 'AX T1 FLAIR.nii.gz')]\n",
    "effective_sequences_TSC = train_source_files_df.loc[(train_source_files_df['Sequences'] == 'MR AX T2 FLAIR.nii.gz')]\n",
    "effective_sequences_Normal = train_source_files_df.loc[(train_source_files_df['Sequences'] == 'AX T2 FLAIR.nii.gz')]\n",
    "train_patient_ids = train_source_files_df.MRID.unique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TSC Seq\")\n",
    "print(train_source_files_df.loc[(train_source_files_df['Sequences'] == 'MR AX T2 FLAIR.nii.gz')].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal Seq\")\n",
    "print(train_source_files_df.loc[(train_source_files_df['Sequences'] == 'AX T2 FLAIR.nii.gz')].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd467607",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef459cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the number of MRI sequesnces/ what kind of MRI sequesces we want!\n",
    "\n",
    "#effective_sequences = source_files_df.loc[(source_files_df['Sequences'] == 'FSPGR BRAVO.nii.gz')]\n",
    "#effective_sequences = source_files_df.loc[(source_files_df['Sequences'] == 'AX T1 FLAIR.nii.gz')]\n",
    "effective_sequences_TSC = train_source_files_df.loc[(train_source_files_df['Sequences'] == 'MR AX T2 FLAIR.nii.gz')]\n",
    "effective_sequences_Normal = train_source_files_df.loc[(train_source_files_df['Sequences'] == 'AX T2 FLAIR.nii.gz')] \n",
    "    \n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#  read all train seqs \n",
    "Train_seqs=[]\n",
    "train_labels=[]\n",
    "train_subjects = []\n",
    "\n",
    "classes={\"TSC\":1, \"Normal\":0}\n",
    "\n",
    "train_patient_id=np.array(effective_sequences_TSC[\"MRID\"])\n",
    "train_seq_name=np.array(effective_sequences_TSC[\"Sequences\"])\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(effective_sequences_TSC[\"Sequences\"]))):    \n",
    "    train_subjects.append(tio.Subject(\n",
    "        mri=tio.ScalarImage(os.path.join(r\"D:/TSC/train_mix/Train/TSC\",train_patient_id[i],train_seq_name[i])),\n",
    "        label=classes[\"TSC\"],\n",
    "        ))\n",
    "    \n",
    "############################################## Normal ########################################################\n",
    "Ntrain_patient_id=np.array(effective_sequences_Normal[\"MRID\"])\n",
    "Ntrain_seq_name=np.array(effective_sequences_Normal[\"Sequences\"])\n",
    "                                        \n",
    "\n",
    "for i in tqdm(range(len(effective_sequences_Normal[\"Sequences\"]))):\n",
    "    train_subjects.append(tio.Subject(\n",
    "        mri=tio.ScalarImage(os.path.join(r\"D:/TSC/train_mix/Train/Normal\",Ntrain_patient_id[i],Ntrain_seq_name[i])),\n",
    "        label=classes[\"Normal\"],\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset size:', len(train_subjects), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_subject = train_subjects[10]\n",
    "one_subject.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_subject)\n",
    "print(one_subject.mri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ee74d",
   "metadata": {},
   "source": [
    "# Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f64dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only augment training data\n",
    "\n",
    "image_out_size = (200,200,100)\n",
    "\n",
    "train_transforms = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.Resample(2), # Isotropic (2,2,2)mm\n",
    "    tio.CropOrPad(image_out_size),\n",
    "#   tio.RandomMotion(p=0.2),\n",
    "#   tio.RandomBiasField(p=0.3),\n",
    "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
    "    tio.RandomNoise(p=0.15),\n",
    "    tio.RandomBlur(p=0.2),\n",
    "    tio.RandomFlip(),\n",
    "    # tio.Lambda(lambda x: patchSample(x))\n",
    "    tio.OneOf({\n",
    "        #tio.RandomAffine(): 0.4,\n",
    "        tio.RandomElasticDeformation(): 0.1,\n",
    "    }),\n",
    "    tio.OneHot()])\n",
    "\n",
    "val_transforms = tio.Compose([\n",
    "        tio.ToCanonical(),\n",
    "        tio.Resample(2),\n",
    "        tio.CropOrPad(image_out_size),\n",
    "        #tio.HistogramStandardization({'mri': landmarks}),\n",
    "        tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
    "        tio.OneHot()]) \n",
    "\n",
    "#Splitting dataset to train, Valid and Test\n",
    "idx = np.random.permutation(len(train_subjects))\n",
    "\n",
    "subjTr =[]\n",
    "subjVa =[]\n",
    "subjTs =[]\n",
    "for c , i in enumerate(idx):\n",
    "    if c < int(0.7*len(train_subjects)):\n",
    "        subjTr.append(train_subjects[i])\n",
    "    elif c>= int(0.7*len(train_subjects)) and  c< int(0.9*len(train_subjects)):\n",
    "        subjVa.append(train_subjects[i])\n",
    "    else:\n",
    "        subjTs.append(train_subjects[i])\n",
    "            \n",
    "\n",
    "# https://torchio.readthedocs.io/data/dataset.html\n",
    "\n",
    "dataTr = tio.SubjectsDataset(subjTr, transform=train_transforms)\n",
    "dataVa = tio.SubjectsDataset(subjVa, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subjTr), len(subjVa), len(subjTs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584aeb3f",
   "metadata": {},
   "source": [
    "# Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloaders = {\n",
    "  'train': DataLoader(dataTr, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "  'val': DataLoader(dataVa, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9c4e8",
   "metadata": {},
   "source": [
    "# 3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87c192",
   "metadata": {},
   "source": [
    "#### 3D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79667bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.InstanceNorm3d(out_channels),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.InstanceNorm3d(in_channels),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Dropout(0.25)\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class = 1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(1, 16)\n",
    "        self.dconv_down2 = double_conv(16, 32)\n",
    "        self.dconv_down3 = double_conv(32, 64)\n",
    "        self.dconv_down4 = double_conv(64, 128)\n",
    "        #self.dconv_down5 = double_conv(128, 256)      \n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1,1,1))       \n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "\n",
    "        conv4 = self.dconv_down4(x)\n",
    "        x1 = self.maxpool(conv4)\n",
    "\n",
    "        avgpool = self.avgpool(x1)\n",
    "        avgpool = avgpool.view(avgpool.size(0), -1)\n",
    "        outC = self.fc(avgpool)\n",
    "        \n",
    "        \n",
    "        return torch.sigmoid(outC)\n",
    "    \n",
    "model = Encoder(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8efbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062d398",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9875800",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "from torchsummary import summary\n",
    "print(summary(model, input_size=(1, 200, 200, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73752f",
   "metadata": {},
   "source": [
    "#### Layer Weight Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75808e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.dconv_down4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d69ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.dconv_down1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c266da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.outC.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7048cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Parameters: \", list(Encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b676055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parm in model.parameters():\n",
    "#     print(parm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, parm in model.named_parameters():\n",
    "#     print(name, '\\t\\t' ,  parm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8a919",
   "metadata": {},
   "source": [
    "### Definition of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of hyperparameters\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# Cross Entropy Loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.001\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer= torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "optimizer= torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model.to(device)\n",
    "def train_model(model, optimizer, save_path, num_epochs=num_epochs):\n",
    "    best_loss = 1e10\n",
    "\n",
    "    epoch_Losses = {k:[] for k in ['train', 'val']}\n",
    "    epoch_Accs = {k:[] for k in ['train', 'val']}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        #for i in tgdm(range(0,(Train_ds, batch_size))\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                # gc.collect()\n",
    "                # torch.cuda.empty_cache()\n",
    "\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            Losses, correct, total = 0.0, 0.0, 0.0\n",
    "            for i, batch in enumerate(dataloaders[phase]):\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                batch_images = batch['mri']['data'].to(device=device, dtype=torch.float)\n",
    "                batch_labels = batch['label'].to(device=device, dtype=torch.float)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "\n",
    "                        batch_labels_pred = model(batch_images)  # Pass batch\n",
    "                        loss = criterion(batch_labels_pred[:, 0], batch_labels)\n",
    "                        loss.backward()    # Calculate Gradient\n",
    "                        optimizer.step()   # Update Weights\n",
    "                        \n",
    "                        l2_lambda = 0.001\n",
    "#                         l2_norm = sum(p.pow(2.0).sum()\n",
    "#                         for p in model.parameters())\n",
    "                        loss = loss + l2_lambda #* l2_norm\n",
    "    \n",
    "                    else:\n",
    "                        batch_labels_pred = model(batch_images)  # Pass batch\n",
    "                        loss = criterion(batch_labels_pred[:, 0], batch_labels)\n",
    "            \n",
    "\n",
    "                # statistics\n",
    "                Losses += loss\n",
    "                pred=batch_labels_pred\n",
    "                batch_labels_pred[pred>0.5]=1\n",
    "                batch_labels_pred[pred<=0.5]=0\n",
    "#                 _, predicted = torch.max(batch_labels_pred.data, 1)\n",
    "                predicted = batch_labels_pred[:, 0] #.clone()\n",
    "                total += batch_labels.size(0)\n",
    "                #print(predicted)\n",
    "                #print(batch_labels)\n",
    "                #print('Current: ', torch.sum(predicted==batch_labels))\n",
    "                correct += predicted.eq(batch_labels.data).sum() # (predicted == batch_labels).sum().item()\n",
    "            #print(correct, total)\n",
    "                \n",
    "\n",
    "            epoch_acc = 100 * correct / total\n",
    "            epoch_loss = Losses/len(dataloaders[phase])\n",
    "            \n",
    "            epoch_Losses[phase].append(epoch_loss.cpu().data.numpy())\n",
    "            epoch_Accs[phase].append(epoch_acc.cpu().data.numpy())\n",
    "            \n",
    "            print(\"Epoch: %d/%d (%s) Loss: %f\\tAcc: %f\" % (epoch, num_epochs, phase, epoch_loss, epoch_acc))\n",
    "            #print(\"Epoch: %d/%d (%s) Loss: %f\" % (epoch, num_epochs, phase, epoch_loss))\n",
    "\n",
    "            # save the model weights\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(f\"saving best model to {save_path}\")\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    #print('Best val loss: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    return model, epoch_Losses, epoch_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade050b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataloaders['train']):\n",
    "#   print(batch['mri']['data'].shape)\n",
    "#   print(batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ecefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, save_path, phase='test'):\n",
    "    \n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "    predictions = np.zeros((0)) \n",
    "    correct = total = 0\n",
    "    for i, batch in enumerate(dataloaders[phase]):\n",
    "        batch_images = batch['mri']['data'].to(device=device, dtype=torch.float)\n",
    "        batch_labels = batch['label'].to(device=device, dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        batch_labels_pred = model(batch_images)  # Pass batch\n",
    "        loss = criterion(batch_labels_pred, batch_labels)\n",
    "        _, predicted = torch.max(batch_labels_pred.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        #predictions = np.concatenate([predictions, predicted.cpu().data.numpy()], 0)\n",
    "    time_elapsed = time.time() - since    \n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    print(predictions.shape)\n",
    "            \n",
    "    print(\"Test tAcc: %f\" % (epoch_acc))    \n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe85e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './pytorch_model/model.pth'\n",
    "model, Loss, Acc = train_model(model, optimizer, save_path, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['train', 'val']:\n",
    "    plt.plot(Loss[key], label=key+' Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Classification Loss')\n",
    "    plt.title('Loss Plot')\n",
    "plt.show()    \n",
    "\n",
    "for key in ['train', 'val']:\n",
    "    plt.plot(Acc[key], label=key+' Acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    plt.title('Accuracy Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d69a24",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f986db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cinfusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        output = model(inputs.cuda()) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ['Normal', 'TSC']\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "plt.ylabel('True label', fontsize=12, color='darkblue') # fontweight='bold'\n",
    "plt.xlabel('Predicted label\\naccuracy={:0.2f}; misclass={:0.2f}', fontsize=12, color='darkblue')\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('output.png')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d14709",
   "metadata": {},
   "source": [
    "# Statistical Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607125f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Statistics and  Class Statistics metrices\n",
    "from pycm import *\n",
    "\n",
    "cm = ConfusionMatrix(y_true, y_pred)\n",
    "cm.table\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(y, pred)\n",
    "auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
    "plt.plot(nn_fpr_keras, nn_tpr_keras, marker='.', label='3D CNN (auc = %0.3f)' % auc_keras)\n",
    "\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "print('Area under ROC curve : ', roc_auc_score(y, pred) *100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab415cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
